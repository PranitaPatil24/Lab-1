{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c383e357-6f34-4da5-a86f-16b57d62926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# ====== Install Required Libraries ======\n",
    "!pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b36cbb-de1f-40b7-9546-1dd3f6d5dc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d844e5-f468-436e-a84e-ce95f6d90a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83f5f7f-ff7d-43a2-9ff4-a7a129f307da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')   # <-- ADD THIS LINE\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8805fc7d-092f-4590-aab8-de307393d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read input text file\n",
    "import os\n",
    "\n",
    "filename = r\"C:\\Lab_1\\ISR\\conflation.txt\"   # <-- use your own text file\n",
    "with open(filename, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edbc4eac-1da8-47de-82a6-04f1a92eff85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Information retrieval systems are designed to find relevant documents.\n",
      "Retrieval is based on matching words between the user's query and stored documents.\n",
      "Different forms of the same word, like connect, connecting, and connection, are reduced to a single root form.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\\n\", text)\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b345cca-3f6a-4f83-b2a4-73ee1335d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4494617-130d-4ec3-82d9-430f4fb22978",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [word for word in tokens if word.isalpha() and word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "451694fb-1a9f-4e70-bc29-a392181cef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e31e6f0a-4d05-4cbc-9767-b785030d431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77508ccb-c694-430f-8ae6-55dae3de6587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming:\n",
      " inform retriev system design find relev document retriev base match word user queri store document differ form word like connect connect connect reduc singl root form\n",
      "--------------------------------------------------------------------------------\n",
      "After Lemmatization:\n",
      " information retrieval system designed find relevant document retrieval based matching word user query stored document different form word like connect connecting connection reduced single root form\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"After Stemming:\\n\", ' '.join(stemmed_words))\n",
    "print(\"-\" * 80)\n",
    "print(\"After Lemmatization:\\n\", ' '.join(lemmatized_words))\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49c5e16d-1262-4a55-93b8-04cae573a828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Representative (Word Frequency):\n",
      "connect: 3\n",
      "retriev: 2\n",
      "document: 2\n",
      "word: 2\n",
      "form: 2\n",
      "inform: 1\n",
      "system: 1\n",
      "design: 1\n",
      "find: 1\n",
      "relev: 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "freq = Counter(stemmed_words)\n",
    "print(\"Document Representative (Word Frequency):\")\n",
    "for word, count in freq.most_common(10):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa46c260-233b-4858-a9fa-f52042b59b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Document representative saved as 'document_representative.txt'\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freq = Counter(stemmed_words)\n",
    "\n",
    "# Save the representative to a text file\n",
    "output_filename = \"document_representative.txt\"\n",
    "with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word, count in freq.most_common():\n",
    "        f.write(f\"{word}: {count}\\n\")\n",
    "\n",
    "print(f\"✅ Document representative saved as '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ae35c-a740-45c4-b16b-561c2b25fe3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
