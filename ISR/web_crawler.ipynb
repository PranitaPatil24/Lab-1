{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e4ce89-752a-4c7c-8dc5-4d3e40aa0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_crawler.py\n",
    "import time\n",
    "import csv\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib import robotparser\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "748f6a45-69fb-4b8c-b67b-7f9bfe21b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://books.toscrape.com/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"MyCrawler/1.0 (+https://example.com/contact) - educational use\"\n",
    "}\n",
    "\n",
    "# polite crawler: check robots.txt\n",
    "def can_fetch(url, user_agent=HEADERS[\"User-Agent\"]):\n",
    "    rp = robotparser.RobotFileParser()\n",
    "    robots_url = urljoin(url, \"/robots.txt\")\n",
    "    rp.set_url(robots_url)\n",
    "    try:\n",
    "        rp.read()\n",
    "    except Exception:\n",
    "        # if robots.txt can't be fetched, default to false to be safe or True if you prefer\n",
    "        return False\n",
    "    return rp.can_fetch(user_agent, url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc99e6b-1421-42ee-ab2d-997f8d23bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_soup(url):\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "    resp.raise_for_status()\n",
    "    return BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "def parse_product_card(card, base_url):\n",
    "    # Example for books.toscrape structure â€” change selectors for your target site\n",
    "    title_tag = card.select_one(\"h3 a\")\n",
    "    title = title_tag[\"title\"].strip()\n",
    "    relative_link = title_tag[\"href\"]\n",
    "    product_url = urljoin(base_url, relative_link)\n",
    "\n",
    "    price = card.select_one(\".price_color\").get_text(strip=True)\n",
    "    availability = card.select_one(\".availability\").get_text(strip=True)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"price\": price,\n",
    "        \"availability\": availability,\n",
    "        \"product_url\": product_url\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1f51e35-e57b-4b3c-97f5-60a7b3ca67b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling page: https://books.toscrape.com/catalogue/page-1.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-2.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-3.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-4.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-5.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-6.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-7.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-8.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-9.html\n",
      "Crawling page: https://books.toscrape.com/catalogue/page-10.html\n",
      "Saved 200 products to books.csv\n"
     ]
    }
   ],
   "source": [
    "def crawl(start_url, max_pages=5, delay=1.0, output_csv=\"products.csv\"):\n",
    "    parsed = urlparse(start_url)\n",
    "    domain_root = f\"{parsed.scheme}://{parsed.netloc}/\"\n",
    "    \n",
    "    if not can_fetch(domain_root):\n",
    "        raise SystemExit(f\"Robots.txt disallows crawling {domain_root} for this user-agent.\")\n",
    "\n",
    "    products = []\n",
    "    next_page = start_url\n",
    "    pages_crawled = 0\n",
    "\n",
    "    while next_page and pages_crawled < max_pages:\n",
    "        print(f\"Crawling page: {next_page}\")\n",
    "        soup = get_soup(next_page)\n",
    "\n",
    "        # Find all product cards\n",
    "        cards = soup.select(\".product_pod\")\n",
    "        for card in cards:\n",
    "            try:\n",
    "                prod = parse_product_card(card, domain_root)\n",
    "                products.append(prod)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to parse product card:\", e)\n",
    "\n",
    "        # Find \"next\" link\n",
    "        next_tag = soup.select_one(\".next a\")\n",
    "        if next_tag:\n",
    "            next_page = urljoin(next_page, next_tag[\"href\"])\n",
    "        else:\n",
    "            next_page = None\n",
    "\n",
    "        pages_crawled += 1\n",
    "        time.sleep(delay)  # polite delay\n",
    "\n",
    "    # Save results\n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"price\", \"availability\", \"product_url\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(products)\n",
    "\n",
    "    print(f\"Saved {len(products)} products to {output_csv}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # start crawling from catalog page\n",
    "    start = \"https://books.toscrape.com/catalogue/page-1.html\"\n",
    "    crawl(start, max_pages=10, delay=1.0, output_csv=\"books.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8981288-9957-46f9-9827-9bb629e78a0f",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
